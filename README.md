add all raw models are in the trained_models folder
download from https://github.com/ultralytics/assets/releases : take n,m,x

all trained model on drive : put in our_retrained_models

our retrained models on the BTSD dataset are in the folder our_retrained_models

test that everything works on a new environment (load the right files, and not the ultralytics librairy)

data in /work/vita/...

use val function of yolo for evaluate performanace of models 

if other data loc, change path in streamloader.py : data_dir = "/work/vita/nmuenger_trinca/annotations/"


# YODSO : YOU ONLY DETECT SIGN ONCE

### instalation:

```
pip instal ...
ultralytics
```


### Data:

Our code is designed to work on the SCITAS clusters. It fetches all the image data from the folder /work/vita/nmuenger_trinca/annotations/. In this folder there are .txt files which indicates the relative path to all the images from a set (the training set for example). As the original yolov8 model expected the data to be located in another folder, the path "/work/vita/nmuenger_trinca/annotations/" is hardcoded in the ultralytics\yolo\data\dataloaders\stream_loaders.py file, line 179. This path is also specified in the .yaml file (named *tsr_dataset_yaml*), where we specify how the dataset should be read. <br>
The BelgianTS dataset contains 210 classes of traffic signs. The defined traffic signs are given in the image at the end of the readme. <br>
For an in-depth explanation of the data format YOLO expects, please refer to this [description](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data). 

### Models:

Before anything, one must first download the models from google drive (https://drive.google.com/drive/folders/1r6YhmoF5XZMKCcJ15O63RUL7nu7NAXp6?usp=sharing) and put them in the correct files. Note that the raw pretrained models can also be downloaded on https://github.com/ultralytics/assets/releases.<br> 
There are two kinds of models. The models pretrained by the Ultralytics team on the COCO dataset (Pretrained-Ultralytics) and our models which we finetuned on the BelgiumTS dataset (42-epochs). Both kind come with 3 different sizes : nano, medium and x-tra large, with performances increasing with model size.

The pretrained models must first be trained with our training script. Our finetuned models can directly be used for inference. 

The pretrained models form ultralytics must be placed in the *trained_models* folder and our finetuned models in the *our_retrained_models* folder.

### Training:

The script to train the network is called our_train.py. One can run it with this command, specifying the model to use and the number of epochs. The model to train must be in the *trained_models* folder.
```
python our_train.py --model yolov8m.pt --epochs 3
```
The model trained and some training metrics are then stored in the *run/detect* folder.

### Inference:

To infer the bounding boxes and classes of an image, we use the *our_inference.py* script. One can specify the model to use, the path to the image (wich must be in the same folder as the path specified in the .yaml file) and the file to output the predictions. An example command is provided below.

```
python our_inference.py --model_path yolov8m_tsd_30epochs.pt --data_path images/01/image.006950.jp2 --output_file our_predictions/prediction.txt
```

This will create the file *prediction.txt* file in the folder *our_predictions* and store all the bounding boxes file in a csv format. The inference time on the image is also displayed.

We also make a quick sript to infer from frames of a video and write in a json file. This sript is nammed *our_video_inference.py* and requires the frames to be place in *work/vita/nmuenger_trinca/annotations/video_frames/* folder in .png format. 

### Testing:

We also provide a test script to compare the performances of the models' different sizes on the whole test set. This script outputs all the results in the terminal and create a plot comparing the performances.


### Disclaimer:

We discovered that not all the sign of the BelgianTS dataset were labeled (see example below). During training, if the network correctly identifies a sign that was not labelled, it is told that it is wrong. We think that this is the reason we didn't get better results, as the yolov8 network is the state-of-the-art and finetuning it should keep the performances high. 

The image below displays an visualisation of this problem. This picture was automatically generated by the YOLO training script and displays the labelisation of some of the images in validation set. Most of the traffic signs are well anotated but we can see that some of the 'one-way street' signs are note labelised at all. 

<p align="center">
<img src="https://github.com/TicaGit/yolov8_tsd/blob/tibo_yolo_retrain/image_read_me/Labels.jpeg" width="800">
</p>

### Results :

Our project goal was to retrain yolov8 and finetune it on the BelgianTS dataset. We obtain our best results with the following method. For 14 epochs, we trained only the last part of the detection head (modules 16, 18, 19 & 21). Then we unfreezed the rest of the head (modules 12 & 15) and continued the training for 14 epochs. We found out on a previous training, that unfreezing the backbone led to a drop in performance. Thus, we decided to keep it frozen and never train, but we did finetune the whole head for another 14 epochs. The architecture of yolov8 and the modules' numbers are shown on the image below. 

<p align="center">
<img src="https://github.com/TicaGit/yolov8_tsd/blob/tibo_yolo_retrain/image_read_me/yolo_archi.png" width="400">
</p>

The performance of the model on the whole test set is shown on the graph below. The three differents sizes are compared, in terms of performance (map50 and map50-95) and inference time. On the second plot, we display the model size in Ko.

<p align="center">
<img src="https://github.com/TicaGit/yolov8_tsd/blob/tibo_yolo_retrain/image_read_me/map_time_with_test.png" width="400">
</p>

<p align="center">
<img src="https://github.com/TicaGit/yolov8_tsd/blob/tibo_yolo_retrain/image_read_me/size_vs_model.png" width="400">
</p>

As an example, we selected an image with multiple signs to detect, and infered the results with the 3 different models. We also compare the time it took to infer the prediction on this image (in milliseconds)

Ground truth 
<p align="center">
<img src="https://github.com/TicaGit/yolov8_tsd/blob/tibo_yolo_retrain/            /.png" width="800">
</p>

Inference with the Nano model 
<p align="center">
<img src="https://github.com/TicaGit/yolov8_tsd/blob/tibo_yolo_retrain/image_read_me/Pred_n.png" width="800">
</p>

Inference with the Medium model
<p align="center">
<img src="https://github.com/TicaGit/yolov8_tsd/blob/tibo_yolo_retrain/image_read_me/Pred_m.png" width="800">
</p>

Inference with the Extra Large model
<p align="center">
<img src="https://github.com/TicaGit/yolov8_tsd/blob/tibo_yolo_retrain/image_read_me/Pred_x.png" width="800">
</p>

Comparion of inference time on one image.
<p align="center">
<img src="https://github.com/TicaGit/yolov8_tsd/blob/tibo_yolo_retrain/image_read_me/time_vs_model.png" width="400">
</p>

### Table with the class names 

<p align="center">
<img src="https://github.com/TicaGit/yolov8_tsd/blob/tibo_yolo_retrain/image_read_me/defined_sign.png" width="400">
</p>







